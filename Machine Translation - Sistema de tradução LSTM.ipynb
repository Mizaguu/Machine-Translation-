{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset: http://www.manythings.org/anki/deu-eng.zip\n",
    "import string\n",
    "import re\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função que abre um arquivo de texto \n",
    "def abre_txt(endereco_arquivo):\n",
    "    arquivo = open(endereco_arquivo, mode='rt', encoding='utf-8') # mode='rt' é para abrir o arquivo no modo leitura\n",
    "    texto = arquivo.read()\n",
    "    arquivo.close()\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função que quebra o texto em uma lista de palavras\n",
    "def lista_palavras(texto):\n",
    "    lista = texto.strip().split('\\n')\n",
    "    lista = [i.split('\\t') for i in lista]\n",
    "    return lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = abre_txt(\"C:/Users/Natan/OneDrive/Documentos/DidaticaTech/deu.txt\")\n",
    "alemao_ingles = lista_palavras(dataset)\n",
    "alemao_ingles = array(alemao_ingles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pegando apenas uma parte do dataset, para poupar processamento:\n",
    "alemao_ingles = alemao_ingles[:50000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alemao_ingles[24000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont=0\n",
    "for i in alemao_ingles[:,0]:\n",
    "    print(i)\n",
    "    cont+=1\n",
    "    if cont>5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo pontuações:\n",
    "alemao_ingles[:,0] = [re.sub('[^\\w\\s]','',s) for s in alemao_ingles[:,0]]\n",
    "alemao_ingles[:,1] = [re.sub('[^\\w\\s]','',s) for s in alemao_ingles[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alemao_ingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando tudo para minúsculas\n",
    "for i in range(len(alemao_ingles)):\n",
    "    alemao_ingles[i,0] = alemao_ingles[i,0].lower()\n",
    "    alemao_ingles[i,1] = alemao_ingles[i,1].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando o tamanho das sentenças em cada idioma:\n",
    "\n",
    "ingles = []\n",
    "alemao = []\n",
    "\n",
    "for i in alemao_ingles[:,0]:\n",
    "    ingles.append(len(i.split()))\n",
    "\n",
    "for i in alemao_ingles[:,1]:\n",
    "    alemao.append(len(i.split()))\n",
    "\n",
    "df = pd.DataFrame({'ingles':ingles, 'alemao':alemao})\n",
    "\n",
    "tamanho_max_ingles = df['ingles'].max()\n",
    "tamanho_max_alemao = df['alemao'].max()\n",
    "\n",
    "print('Máximo comprimento inglês:', tamanho_max_ingles)\n",
    "print('Máximo comprimento alemão:', tamanho_max_alemao)\n",
    "\n",
    "# Visualizando um histograma de cada idioma:\n",
    "df.hist(bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função que tokeniza as palavras:\n",
    "def tokenizador(frases):\n",
    "    tokenizador = Tokenizer()\n",
    "    tokenizador.fit_on_texts(frases)\n",
    "    return tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizador inglês\n",
    "tokenizador_ingles = tokenizador(alemao_ingles[:, 0])\n",
    "tamanho_vocabulario_ingles = len(tokenizador_ingles.word_index) + 1\n",
    "\n",
    "print('Tamanho do vocabulário inglês:', tamanho_vocabulario_ingles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizador alemão\n",
    "tokenizador_alemao = tokenizador(alemao_ingles[:, 1])\n",
    "tamanho_vocabulario_alemao = len(tokenizador_alemao.word_index) + 1\n",
    "\n",
    "print('Tamanho do vocabulário alemão:', tamanho_vocabulario_alemao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tokenização já foi preparada para o dataset inteiro, agora só falta aplicar ela em cada subgrupo de treino e teste.\n",
    "# Primeiro vamos dividir o dataset entre treino e teste, depois vamos tokenizar cada idioma, em cada grupo (treino e teste). \n",
    "# Ao fazer isso, pode ser que o comprimento máximo das sentenças do treino inglês fique diferente do seu teste, afinal esse \n",
    "# comprimento será definido a partir do tamanho máximo encontrado no conjunto que estiver sendo tokenizado naquele momento. \n",
    "# Por isso, iremos usar o parâmetro maxlen dentro de pad_sequences, que irá determinar o tamanho máximo independentemente do \n",
    "# tamanho da máxima sentença no dataset que estiver sendo informado. \n",
    "# Nesse caso vou usar o tamanho máximo total inglês para o dataset inglês e vou usar o máximo do alemão com o dataset alemão. \n",
    "# Obviamente, muitas amostras ficarão com valores zero em algumas entradas. Posso escolher se quero deixar esses zeros\n",
    "# à esquerda ou à direita da sentença. Deixar os zeros à direita é melhor porque assim a frase sempre vai começar no mesmo ponto,\n",
    "# e quando chegarem os zeros é sinal de que a frase terminou. Para definir isso, basta usar o parâmetro padding='post' em vez de\n",
    "# padding = 'pre' que é o default.\n",
    "\n",
    "def encoder_frases(tokenizador, comprimento, frases):\n",
    "    frases_tokenizadas = tokenizador.texts_to_sequences(frases)\n",
    "    frases_tokenizadas = pad_sequences(frases_tokenizadas, maxlen=comprimento, padding='post')\n",
    "    return frases_tokenizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "treino, teste = train_test_split(alemao_ingles, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colocando os textos em alemão como variáveis preditoras, e os textos em inglês como variáveis target:\n",
    "x_treino = encoder_frases(tokenizador_alemao, tamanho_max_alemao, treino[:, 1])\n",
    "y_treino = encoder_frases(tokenizador_ingles, tamanho_max_ingles, treino[:, 0])\n",
    "\n",
    "x_teste = encoder_frases(tokenizador_alemao, tamanho_max_alemao, teste[:, 1])\n",
    "y_teste = encoder_frases(tokenizador_ingles, tamanho_max_ingles, teste[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_treino.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_treino.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o modelo de tradução com LSTMs:\n",
    "modelo = Sequential()\n",
    "modelo.add(Embedding(input_dim=tamanho_vocabulario_alemao, output_dim=500, input_length=tamanho_max_alemao))\n",
    "modelo.add(LSTM(500))\n",
    "modelo.add(RepeatVector(tamanho_max_ingles))\n",
    "modelo.add(LSTM(500, return_sequences=True))\n",
    "modelo.add(Dense(tamanho_vocabulario_ingles, activation='softmax'))\n",
    "\n",
    "otimizador = optimizers.RMSprop(lr=0.001)\n",
    "modelo.compile(optimizer=otimizador, loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# A função de custo sparse_categorical_crossentropy é semelhante à categorical_crossentropy, mas ela permite que a variável\n",
    "# target seja utilizada da forma como colocamos, sem precisar estar no formato one-hot-encoding. Isso ajuda a otimizar memória, \n",
    "# já que o vocabulário de saída é muito grande. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historico = modelo.fit(x_treino, y_treino.reshape(y_treino.shape[0], y_treino.shape[1], 1), epochs=40, batch_size=500, \n",
    "                       validation_split = 0.2, verbose=1)\n",
    "# Obs: y_treino precisa estar no formado (amostras, timesteps, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrando a evolução do treinamento:\n",
    "plt.plot(historico.history['loss'])\n",
    "plt.plot(historico.history['val_loss'])\n",
    "plt.legend(['treino','teste'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo predições:\n",
    "previsoes = modelo.predict_classes(x_teste.reshape((x_teste.shape[0],x_teste.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma função que retorna as palavras a partir de seus números de tokenização:\n",
    "def coleta_palavra(token, tokenizador):\n",
    "    for palavra, index in tokenizador.word_index.items():\n",
    "        if index == token:\n",
    "            return palavra\n",
    "    return None\n",
    "\n",
    "# Transformando todas as predições em palavras:\n",
    "texto_previsto = []\n",
    "for frase in previsoes: # para cada frase prevista\n",
    "    sentenca = []\n",
    "    for token in range(len(frase)): # para cada token previsto dentro de uma frase\n",
    "        palavra_ingles = coleta_palavra(frase[token], tokenizador_ingles) # palavra inglês\n",
    "        if(palavra_ingles == None):\n",
    "            sentenca.append('')\n",
    "        else:\n",
    "            sentenca.append(palavra_ingles)\n",
    "\n",
    "    texto_previsto.append(' '.join(sentenca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um dataset que contém as frases em inglês do dataset de teste e as respectivas previsões do modelo:\n",
    "df_previsoes = pd.DataFrame({'referências' : teste[:,0], 'previsões' : texto_previsto})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando alguns resultados:\n",
    "df_previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mostrando aleatoriamente 10 frases do dataset:\n",
    "df_previsoes.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk\n",
    "# Aprendendo a calcular o BLEU SCORE em uma sentença:\n",
    "from nltk.translate.bleu_score import sentence_bleu # usado para calcular uma sentença\n",
    "referencias = [['ele', 'leu', 'o', 'texto'], ['ele', 'leu', 'um', 'livro']]\n",
    "traducao = ['ele', 'leu', 'o', 'artigo']\n",
    "\n",
    "score = sentence_bleu(referencias, traducao, weights=(0, 0, 0, 1))\n",
    "print(score)\n",
    "# Se quero avaliar uma palavra por vez, devo passar o peso 1 no primeiro item (1, 0, 0, 0). Se quero que avalie por grupos de 2\n",
    "# palavras (bigram), preciso passar o peso 1 no segundo item (0, 1, 0, 0), e assim por diante. Posso avaliar até 4 palavras em \n",
    "# sequência (4-gram).\n",
    "# Se quiser, posso considerar a avaliação de vários n-grams ao mesmo tempo, passando o peso de 0.25 para os 4, ou 0.33 para três\n",
    "# deles, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando o BLEU SCORE em várias sentenças:\n",
    "from nltk.translate.bleu_score import corpus_bleu # usado para calcular várias sentenças\n",
    "\n",
    "referencias = [[['ele', 'leu', 'o', 'texto'], ['ele', 'leu', 'um', 'livro']], [['ela', 'joga', 'xadrez']]]\n",
    "candidatos = [['ele', 'leu', 'o', 'artigo'], ['ela', 'quer', 'um', 'lanche']]\n",
    "\n",
    "score = corpus_bleu(referencias, candidatos, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "print(score)\n",
    "\n",
    "# Repare que preciso informar listas dentro de listas. Nos candidatos, cada lista será comparada com sua respectiva referência, \n",
    "# por isso há várias listas de candidatos. \n",
    "# Nas referências, existe um agrupamento a mais porque podemos ter mais de uma referência para o mesmo candidato. Mas a lógica\n",
    "# é a mesma, temos uma lista de referências para cada candidato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando nossos resultados em listas para aplicar o corpus_bleu:\n",
    "candidatos = []\n",
    "for i in df_previsoes['previsões']:\n",
    "    candidatos.append(i.split()) # i.split() irá quebrar cada frase em uma lista de palavras individuais. Essa lista será colocada dentro de outra lista (candidatos)\n",
    "\n",
    "referencias = []\n",
    "for i in df_previsoes['referências']:\n",
    "    lista = [i.split()]\n",
    "    referencias.append(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "referencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = corpus_bleu(referencias, candidatos, weights=(0.5, 0.25, 0.25, 0))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
